"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å —Å–∫—Ä–∞–ø–µ—Ä–∞
"""
import logging
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from datetime import datetime
import asyncio
import aiohttp
from bs4 import BeautifulSoup


class BaseScraper(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —Å–∫—Ä–∞–ø–µ—Ä–æ–≤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.config.get('timeout', 30)),
            headers={
                'User-Agent': self.config.get('user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        if self.session:
            await self.session.close()
    
    @abstractmethod
    async def scrape_page(self, url: str) -> Dict[str, Any]:
        """–°–∫—Ä–∞–ø–∏—Ç—å –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        pass
    
    @abstractmethod
    async def parse_content(self, html: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ HTML"""
        pass
    
    @abstractmethod
    async def get_next_page_url(self, html: str, current_url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        pass
    
    async def scrape_with_pagination(self, start_url: str, max_pages: int = 10) -> List[Dict[str, Any]]:
        """–°–∫—Ä–∞–ø–∏—Ç—å —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
        if not self.session:
            raise RuntimeError("–°–∫—Ä–∞–ø–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ async with.")
        
        all_data = []
        current_url = start_url
        page_count = 0
        
        try:
            while current_url and page_count < max_pages:
                self.logger.info(f"üîÑ –°–∫—Ä–∞–ø–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_count + 1}: {current_url}")
                
                # –°–∫—Ä–∞–ø–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                page_data = await self.scrape_page(current_url)
                
                if page_data.get('success'):
                    content = page_data.get('content', '')
                    parsed_items = await self.parse_content(content)
                    all_data.extend(parsed_items)
                    
                    self.logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(parsed_items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_count + 1}")
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    current_url = await self.get_next_page_url(content, current_url)
                    page_count += 1
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    if current_url:
                        delay = self.config.get('delay', 1.0)
                        await asyncio.sleep(delay)
                else:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_data.get('error')}")
                    break
            
            self.logger.info(f"‚úÖ –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {page_count}, —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(all_data)}")
            return all_data
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
            return all_data
    
    async def make_request(self, url: str) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å HTTP –∑–∞–ø—Ä–æ—Å"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    return {
                        'success': True,
                        'content': content,
                        'status_code': response.status,
                        'url': url
                    }
                else:
                    return {
                        'success': False,
                        'error': f"HTTP {response.status}",
                        'status_code': response.status,
                        'url': url
                    }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'url': url
            }
    
    def log_scraping_stats(self, total_items: int, total_pages: int, duration: float):
        """–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
        self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞:")
        self.logger.info(f"   –°—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_pages}")
        self.logger.info(f"   –≠–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {total_items}")
        self.logger.info(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f}—Å")
        self.logger.info(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {total_items/duration:.2f} —ç–ª–µ–º–µ–Ω—Ç–æ–≤/—Å–µ–∫") 