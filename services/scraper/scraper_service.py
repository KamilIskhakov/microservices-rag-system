"""
Scraper Service - –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –ú–∏–Ω—é—Å—Ç–∞
"""
import asyncio
import logging
import time
from datetime import datetime
from typing import Dict, Any, Optional, List

import redis
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

from src.domain.entities.scraped_data import ScrapedData
from src.infrastructure.persistence.file_scraper_repository import FileScraperRepository as ScraperRepository
from src.application.commands.scraper_commands import (
    StartScrapingCommand,
    StartScrapingCommandHandler,
    GetScrapingStatusCommand,
    GetScrapingStatusCommandHandler,
    CommandBus
)
from src.shared.utils.logger import logger

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logger

app = FastAPI(title="RAG Scraper Service", version="2.0.0")

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis
redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
scraper_repository: Optional[ScraperRepository] = None
command_bus: Optional[CommandBus] = None

class ScrapingRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    source: str = "minjust"  # –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö
    force_update: bool = False  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    max_items: Optional[int] = None  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤

class ScrapingResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    success: bool
    job_id: Optional[str] = None
    message: str = ""
    timestamp: str
    error: Optional[str] = None

class ScrapingStatus(BaseModel):
    """–°—Ç–∞—Ç—É—Å –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    job_id: str
    status: str  # "running", "completed", "failed"
    progress: float = 0.0
    items_processed: int = 0
    items_total: int = 0
    start_time: str = ""
    end_time: Optional[str] = None
    error: Optional[str] = None

class HealthResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è"""
    status: str
    service: str
    timestamp: str
    dependencies: Dict[str, str]

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    global scraper_repository, command_bus
    
    try:
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Scraper Service...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–∫—Ä–∞–ø–µ—Ä–∞
        scraper_repository = ScraperRepository()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —à–∏–Ω—É –∫–æ–º–∞–Ω–¥
        command_bus = CommandBus()
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥
        command_bus.register_handler(
            StartScrapingCommand,
            StartScrapingCommandHandler(scraper_repository)
        )
        
        command_bus.register_handler(
            GetScrapingStatusCommand,
            GetScrapingStatusCommandHandler(scraper_repository)
        )
        
        logger.info("‚úÖ Scraper Service –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Scraper Service: {e}")
        raise

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Redis
        redis_healthy = "healthy"
        try:
            redis_client.ping()
        except:
            redis_healthy = "unhealthy"
        
        dependencies = {
            "redis": redis_healthy,
            "minjust_api": "unknown"  # –ë—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å—Å—è —á–µ—Ä–µ–∑ HTTP
        }
        
        return HealthResponse(
            status="healthy" if redis_healthy == "healthy" else "unhealthy",
            service="scraper",
            timestamp=datetime.now().isoformat(),
            dependencies=dependencies
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return HealthResponse(
            status="unhealthy",
            service="scraper",
            timestamp=datetime.now().isoformat(),
            dependencies={"error": str(e)}
        )

@app.post("/start-scraping", response_model=ScrapingResponse)
async def start_scraping(request: ScrapingRequest, background_tasks: BackgroundTasks):
    """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        logger.info(f"üï∑Ô∏è –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {request.source}")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
        command = StartScrapingCommand(
            source=request.source,
            force_update=request.force_update,
            max_items=request.max_items
        )
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–º–∞–Ω–¥—É —á–µ—Ä–µ–∑ —à–∏–Ω—É –∫–æ–º–∞–Ω–¥
        result = await command_bus.handle(command)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –≤ —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
        background_tasks.add_task(run_scraping_job, result.job_id)
        
        return ScrapingResponse(
            success=True,
            job_id=result.job_id,
            message=f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω —Å job_id: {result.job_id}",
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        
        return ScrapingResponse(
            success=False,
            message="–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞",
            timestamp=datetime.now().isoformat(),
            error=str(e)
        )

async def run_scraping_job(job_id: str):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ —Ñ–æ–Ω–µ"""
    try:
        logger.info(f"üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ {job_id}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "running"
        scraper_repository.update_job_status(job_id, "running")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä
        from minjust_scraper import MinjustScraper
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä—Å–∏–Ω–≥
        scraper = MinjustScraper()
        materials = scraper.scrape_to_scraped_data()  # –ü–∞—Ä—Å–∏–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(materials)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π...")
        for i, material in enumerate(materials):
            scraper_repository.save_data(material)
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 100 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            if i % 100 == 0:
                progress = (i + 1) / len(materials) * 0.5  # 50% –Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                scraper_repository.update_job_status(job_id, "running", progress=progress, items_processed=i+1, items_total=len(materials))
        
        # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º —Å RAG —Å–∏—Å—Ç–µ–º–æ–π —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        from config import scraper_settings
        if scraper_settings.RAG_INTEGRATION_ENABLED:
            logger.info(f"üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å RAG —Å–∏—Å—Ç–µ–º–æ–π –¥–ª—è {len(materials)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤...")
            from rag_integration import RAGIntegration
            rag_integration = RAGIntegration()
            await rag_integration.add_scraped_data_to_rag(materials)
            logger.info("‚úÖ RAG –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        else:
            logger.info("RAG integration disabled, skipping vector store update")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
        scraper_repository.update_job_status(job_id, "completed", progress=1.0, items_processed=len(materials), items_total=len(materials))
        
        logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {job_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(materials)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ {job_id}: {e}")
        scraper_repository.update_job_status(job_id, "failed")

@app.get("/status/{job_id}", response_model=ScrapingStatus)
async def get_scraping_status(job_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    try:
        logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {job_id}")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
        command = GetScrapingStatusCommand(job_id=job_id)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–º–∞–Ω–¥—É —á–µ—Ä–µ–∑ —à–∏–Ω—É –∫–æ–º–∞–Ω–¥
        result = await command_bus.handle(command)
        
        return ScrapingStatus(
            job_id=job_id,
            status=result.status,
            progress=result.progress,
            items_processed=result.items_processed,
            items_total=result.items_total,
            start_time=result.start_time,
            end_time=result.end_time,
            error=result.error
        )
        
    except Exception as e:
        logger.error(f"Error getting scraping status for job {job_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/jobs")
async def get_all_jobs():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–¥–∞—á –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    try:
        jobs = scraper_repository.get_all_jobs()
        
        return {
            "success": True,
            "jobs": [
                {
                    "job_id": job.job_id,
                    "status": job.status,
                    "source": job.source,
                    "created_at": job.created_at.isoformat(),
                    "completed_at": job.completed_at.isoformat() if job.completed_at else None
                }
                for job in jobs
            ],
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting all jobs: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/data/latest")
async def get_latest_data():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    try:
        data = scraper_repository.get_latest_data()
        
        return {
            "success": True,
            "data": [
                {
                    "id": item.id,
                    "title": item.title,
                    "court_name": item.court_name,
                    "court_date": item.court_date.isoformat(),
                    "material_name": item.material_name,
                    "created_at": item.created_at.isoformat()
                }
                for item in data
            ],
            "count": len(data),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting latest data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/data/count")
async def get_data_count():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        count = scraper_repository.get_data_count()
        
        return {
            "success": True,
            "count": count,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting data count: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/data/export")
async def export_data():
    """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö"""
    try:
        logger.info("üì§ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        
        return {
            "success": True,
            "message": "–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—â–µ–Ω",
            "export_id": "export_123",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error exporting data: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/jobs/{job_id}")
async def cancel_job(job_id: str):
    """–û—Ç–º–µ–Ω–∞ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    try:
        success = scraper_repository.cancel_job(job_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="Job not found")
        
        return {
            "success": True,
            "message": f"Job {job_id} cancelled successfully",
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error cancelling job {job_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/statistics")
async def get_statistics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∫—Ä–∞–ø–µ—Ä–∞"""
    try:
        stats = scraper_repository.get_statistics()
        
        return {
            "success": True,
            "statistics": stats,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting statistics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/rag/sync-all")
async def sync_all_to_rag():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å RAG —Å–∏—Å—Ç–µ–º–æ–π –±–∞—Ç—á–∞–º–∏"""
    try:
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å RAG —Å–∏—Å—Ç–µ–º–æ–π...")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        all_data = scraper_repository.get_all_data()  # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        
        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(all_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
        
        if not all_data:
            return {
                "success": False,
                "message": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏",
                "timestamp": datetime.now().isoformat()
            }
        
        # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º —Å RAG —Å–∏—Å—Ç–µ–º–æ–π
        from config import scraper_settings
        if scraper_settings.RAG_INTEGRATION_ENABLED:
            from rag_integration import RAGIntegration
            rag_integration = RAGIntegration()
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ 100 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            batch_size = 100
            total_batches = (len(all_data) + batch_size - 1) // batch_size
            total_synced = 0
            
            for i in range(0, len(all_data), batch_size):
                batch = all_data[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                
                logger.info(f"üì¶ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á {batch_num}/{total_batches} ({len(batch)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –±–∞—Ç—á
                success = await rag_integration.add_scraped_data_to_rag(batch)
                
                if success:
                    total_synced += len(batch)
                    logger.info(f"‚úÖ –ë–∞—Ç—á {batch_num} —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω ({total_synced}/{len(all_data)})")
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –±–∞—Ç—á–∞ {batch_num}")
                    return {
                        "success": False,
                        "message": f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –±–∞—Ç—á–∞ {batch_num}",
                        "timestamp": datetime.now().isoformat()
                    }
            
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {total_synced} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å RAG")
            return {
                "success": True,
                "message": f"–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {total_synced} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ {total_batches} –±–∞—Ç—á–∞—Ö",
                "documents_count": total_synced,
                "batches_processed": total_batches,
                "timestamp": datetime.now().isoformat()
            }
        else:
            logger.warning("RAG integration disabled")
            return {
                "success": False,
                "message": "RAG integration disabled",
                "timestamp": datetime.now().isoformat()
            }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —Å RAG: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001) 