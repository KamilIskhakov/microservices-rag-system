"""
Scraper Service API - –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å
"""
import logging
import time
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional
import os
import sys

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –¥–æ–º–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–µ
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# –ò–º–ø–æ—Ä—Ç—ã –¥–æ–º–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–∏
from domain.services.scraper_service import ScraperService
from infrastructure.persistence.in_memory_repository import InMemoryScraperRepository

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(title="Scraper Service", version="2.0.0")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
scraper_service: Optional[ScraperService] = None


class CreateJobRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    source_url: str
    job_type: str = "minjust"


class JobResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –¥–ª—è –∑–∞–¥–∞—á–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    success: bool
    job_id: Optional[str] = None
    data_id: Optional[str] = None
    status: str
    processing_time: float
    timestamp: str
    error: Optional[str] = None


class ScrapedDataInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    id: str
    source_url: str
    title: Optional[str] = None
    status: str
    created_at: str
    updated_at: str
    content_length: int
    error: Optional[str] = None


class ScrapingJobInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–¥–∞—á–µ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    id: str
    source_url: str
    job_type: str
    status: str
    priority: int
    created_at: str
    updated_at: str
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    error: Optional[str] = None


class StatisticsResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
    total_scraped_data: int
    processed_data: int
    failed_data: int
    pending_data: int
    total_jobs: int
    completed_jobs: int
    failed_jobs: int
    pending_jobs: int
    running_jobs: int


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    global scraper_service
    
    try:
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Scraper Service...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
        scraper_repository = InMemoryScraperRepository()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–º–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å
        scraper_service = ScraperService(scraper_repository)
        
        logger.info("‚úÖ Scraper Service –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Scraper Service: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
    global scraper_service
    
    try:
        if scraper_service:
            await scraper_service.close()
        logger.info("‚úÖ Scraper Service –∑–∞–≤–µ—Ä—à–µ–Ω")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Scraper Service: {e}")


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        if scraper_service is None:
            return {"status": "unhealthy", "error": "Service not initialized"}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞
        stats = scraper_service.get_statistics()
        
        return {
            "status": "healthy",
            "service": "scraper",
            "timestamp": datetime.now().isoformat(),
            "total_jobs": stats["total_jobs"],
            "total_scraped_data": stats["total_scraped_data"]
        }
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {"status": "unhealthy", "error": str(e)}


@app.post("/create-job", response_model=JobResponse)
async def create_scraping_job(request: CreateJobRequest):
    """–°–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        start_time = time.time()
        
        logger.info(f"–°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –¥–ª—è: {request.source_url}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É
        job = await scraper_service.create_scraping_job(
            source_url=request.source_url,
            job_type=request.job_type
        )
        
        processing_time = time.time() - start_time
        
        return JobResponse(
            success=True,
            job_id=job.id,
            status="created",
            processing_time=processing_time,
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
        
        return JobResponse(
            success=False,
            processing_time=processing_time,
            timestamp=datetime.now().isoformat(),
            error=str(e)
        )


@app.post("/execute-job/{job_id}", response_model=JobResponse)
async def execute_scraping_job(job_id: str, background_tasks: BackgroundTasks):
    """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        start_time = time.time()
        
        logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á—É —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {job_id}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á—É
        result = await scraper_service.execute_scraping_job(job_id)
        
        processing_time = time.time() - start_time
        
        return JobResponse(
            success=result["success"],
            job_id=result["job_id"],
            data_id=result.get("data_id"),
            status=result["status"],
            processing_time=processing_time,
            timestamp=datetime.now().isoformat(),
            error=result.get("error")
        )
        
    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
        
        return JobResponse(
            success=False,
            job_id=job_id,
            processing_time=processing_time,
            timestamp=datetime.now().isoformat(),
            error=str(e)
        )


@app.get("/job/{job_id}", response_model=ScrapingJobInfo)
async def get_scraping_job(job_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–¥–∞—á—É —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –ø–æ ID"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        job = scraper_service.get_scraping_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        
        return ScrapingJobInfo(
            id=job.id,
            source_url=job.source_url,
            job_type=job.job_type,
            status=job.status,
            priority=job.priority,
            created_at=job.created_at.isoformat(),
            updated_at=job.updated_at.isoformat(),
            started_at=job.started_at.isoformat() if job.started_at else None,
            completed_at=job.completed_at.isoformat() if job.completed_at else None,
            error=job.error
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/data/{data_id}", response_model=ScrapedDataInfo)
async def get_scraped_data(data_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ ID"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        data = scraper_service.get_scraped_data(data_id)
        if not data:
            raise HTTPException(status_code=404, detail="Data not found")
        
        return ScrapedDataInfo(
            id=data.id,
            source_url=data.source_url,
            title=data.title,
            status=data.status,
            created_at=data.created_at.isoformat(),
            updated_at=data.updated_at.isoformat(),
            content_length=len(data.content),
            error=data.error
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/jobs/pending")
async def get_pending_jobs():
    """–ü–æ–ª—É—á–∏—Ç—å –æ–∂–∏–¥–∞—é—â–∏–µ –∑–∞–¥–∞—á–∏"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        jobs = scraper_service.get_pending_jobs()
        
        result = []
        for job in jobs:
            result.append({
                "id": job.id,
                "source_url": job.source_url,
                "job_type": job.job_type,
                "status": job.status,
                "created_at": job.created_at.isoformat()
            })
        
        return {
            "pending_jobs": result,
            "total": len(result)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–∂–∏–¥–∞—é—â–∏—Ö –∑–∞–¥–∞—á: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/jobs/running")
async def get_running_jobs():
    """–ü–æ–ª—É—á–∏—Ç—å –≤—ã–ø–æ–ª–Ω—è—é—â–∏–µ—Å—è –∑–∞–¥–∞—á–∏"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        jobs = scraper_service.get_running_jobs()
        
        result = []
        for job in jobs:
            result.append({
                "id": job.id,
                "source_url": job.source_url,
                "job_type": job.job_type,
                "status": job.status,
                "started_at": job.started_at.isoformat() if job.started_at else None
            })
        
        return {
            "running_jobs": result,
            "total": len(result)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω—è—é—â–∏—Ö—Å—è –∑–∞–¥–∞—á: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/data")
async def get_all_scraped_data(limit: int = 100):
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Å–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        data_list = scraper_service.get_all_scraped_data(limit)
        
        result = []
        for data in data_list:
            result.append({
                "id": data.id,
                "source_url": data.source_url,
                "title": data.title,
                "status": data.status,
                "created_at": data.created_at.isoformat(),
                "content_length": len(data.content)
            })
        
        return {
            "scraped_data": result,
            "total": len(result)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/statistics", response_model=StatisticsResponse)
async def get_statistics():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        stats = scraper_service.get_statistics()
        
        return StatisticsResponse(**stats)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/job/{job_id}")
async def delete_scraping_job(job_id: str):
    """–£–¥–∞–ª–∏—Ç—å –∑–∞–¥–∞—á—É —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        success = scraper_service.delete_scraping_job(job_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="Job not found")
        
        return {
            "success": True,
            "job_id": job_id,
            "message": "Job deleted successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/data/{data_id}")
async def delete_scraped_data(data_id: str):
    """–£–¥–∞–ª–∏—Ç—å —Å–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    try:
        if scraper_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        success = scraper_service.delete_scraped_data(data_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="Data not found")
        
        return {
            "success": True,
            "data_id": data_id,
            "message": "Data deleted successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
