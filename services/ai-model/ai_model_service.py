"""
AI Model Service - FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–æ–º–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É –∏–∑ src/
"""
import asyncio
import logging
import time
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from datetime import datetime
from typing import Dict, Any, List, Optional
import os
import sys

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import psutil

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –¥–æ–º–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–∏
sys.path.append(os.path.join(os.path.dirname(__file__), '../../src'))

# –ò–º–ø–æ—Ä—Ç—ã –¥–æ–º–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–∏
from src.domain.services.model_service import ModelService
from src.infrastructure.persistence.optimized_model_repository import OptimizedModelRepository
from src.domain.observers.model_observer import ModelEventManager


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(title="AI Model Service", version="1.0.0")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
model_service: Optional[ModelService] = None
event_manager: Optional[ModelEventManager] = None

# –ü—É–ª –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
thread_pool: Optional[ThreadPoolExecutor] = None
process_pool: Optional[ProcessPoolExecutor] = None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
def generate_text_in_process(model_path: str, prompt: str, max_length: int, temperature: float) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–ª—è –æ–±—Ö–æ–¥–∞ GIL"""
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float32)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è CPU
    torch.set_num_threads(4)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512)
    
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=10,
            do_sample=True,
            temperature=0.1,
            pad_token_id=tokenizer.eos_token_id,
            num_beams=1,
            repetition_penalty=1.0,
            use_cache=True,
            eos_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
from src.shared.config.config_manager import ConfigManager
from src.infrastructure.factories.device_factory import DeviceFactory

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
config_manager = ConfigManager()
device_factory = DeviceFactory(config_manager)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
MAX_WORKERS = config_manager.get_max_workers()
MAX_PROCESSES = config_manager.get_max_processes()


class ModelRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
    query: str
    context: List[str] = []
    max_length: int = 512
    temperature: float = 0.7
    model_id: str = "qwen-model_full"
    use_async: bool = True


class ModelResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏"""
    success: bool
    result: str
    processing_time: float
    timestamp: str
    model_id: Optional[str] = None
    error: Optional[str] = None
    memory_usage: Optional[Dict[str, Any]] = None


class ModelInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    model_id: str
    name: str
    type: str
    device: str
    is_loaded: bool
    memory_usage: Optional[Dict[str, Any]] = None


class SystemInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ"""
    cpu_count: int
    memory_total: float
    memory_available: float
    memory_percent: float
    thread_pool_workers: int
    process_pool_workers: int
    loaded_models_count: int


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    global model_service, event_manager, thread_pool, process_pool
    
    try:
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI Model Service...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É–ª—ã –ø–æ—Ç–æ–∫–æ–≤ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        thread_pool = ThreadPoolExecutor(max_workers=MAX_WORKERS, thread_name_prefix="ai_worker")
        process_pool = ProcessPoolExecutor(max_workers=MAX_PROCESSES)
        
        logger.info(f"üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø—É–ª—ã: ThreadPool({MAX_WORKERS}), ProcessPool({MAX_PROCESSES})")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä —Å–æ–±—ã—Ç–∏–π
        event_manager = ModelEventManager()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ —Å–µ—Ä–≤–∏—Å
        model_repository = OptimizedModelRepository(event_manager)
        model_service = ModelService(model_repository)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º FAISS —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
        # vector_repository = FaissVectorRepository() # This line is removed
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_manager.log_configuration()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        device_validation = device_factory.validate_all_devices()
        logger.info(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {device_validation}")
        
        logger.info("‚úÖ AI Model Service –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        try:
            logger.info("üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é...")
            model_name = config_manager.get_model_config()["name"]
            device_type = config_manager.get_optimal_device()
            await load_model(model_name, device_type)
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –≤—ã–∫–ª—é—á–µ–Ω–∏–∏"""
    global thread_pool, process_pool
    
    logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã AI Model Service...")
    
    if thread_pool:
        thread_pool.shutdown(wait=True)
        logger.info("‚úÖ ThreadPool –∑–∞–≤–µ—Ä—à–µ–Ω")
    
    if process_pool:
        process_pool.shutdown(wait=True)
        logger.info("‚úÖ ProcessPool –∑–∞–≤–µ—Ä—à–µ–Ω")


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        if model_service is None:
            return {"status": "unhealthy", "service": "ai-model", "error": "Service not initialized"}
        
        loaded_models = model_service.get_loaded_models()
        memory_usage = model_service.get_memory_usage()
        
        return {
            "status": "healthy",
            "service": "ai-model",
            "loaded_models_count": len(loaded_models),
            "memory_usage": memory_usage,
            "thread_pool_active": thread_pool._work_queue.qsize() if thread_pool else 0,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {"status": "unhealthy", "service": "ai-model", "error": str(e)}


@app.get("/system-info", response_model=SystemInfo)
async def get_system_info():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ"""
    try:
        memory_info = psutil.virtual_memory()
        loaded_models = model_service.get_loaded_models() if model_service else []
        
        return SystemInfo(
            cpu_count=os.cpu_count() or 1,
            memory_total=memory_info.total / (1024**3),
            memory_available=memory_info.available / (1024**3),
            memory_percent=memory_info.percent,
            thread_pool_workers=MAX_WORKERS,
            process_pool_workers=MAX_PROCESSES,
            loaded_models_count=len(loaded_models)
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/load-model")
async def load_model(model_id: str = "qwen-model_full", device: str = "auto"):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
    try:
        if model_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        model_path = os.environ.get('QWEN_MODEL_PATH', '/app/models/qwen-model_full')
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        loop = asyncio.get_event_loop()
        model = await loop.run_in_executor(thread_pool, model_service.model_repository.load_model, model_path, device)
        
        return {
            "success": True,
            "model_id": model.id,
            "message": f"–ú–æ–¥–µ–ª—å {model.name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ"
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/unload-model")
async def unload_model(model_id: str):
    """–í—ã–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
    try:
        if model_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—ã–≥—Ä—É–∑–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        loop = asyncio.get_event_loop()
        success = await loop.run_in_executor(thread_pool, model_service.unload_model, model_id)
        
        if success:
            return {
                "success": True,
                "message": f"–ú–æ–¥–µ–ª—å {model_id} –≤—ã–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ"
            }
        else:
            raise HTTPException(status_code=404, detail=f"Model {model_id} not found or not loaded")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def generate_response_async(model_id: str, prompt: str, max_length: int, temperature: float) -> str:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π CPU"""
    loop = asyncio.get_event_loop()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor —Å torch.set_num_threads
    result = await loop.run_in_executor(
        thread_pool, 
        model_service.model_repository.generate_text,
        model_id, prompt, max_length, temperature
    )
    return result


@app.post("/generate", response_model=ModelResponse)
async def generate_response(request: ModelRequest):
    """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é"""
    start_time = time.time()
    
    try:
        if model_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        if not model_service.is_model_available(request.model_id):
            raise HTTPException(status_code=503, detail=f"Model {request.model_id} not loaded")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å —Ç–∞–π–º–∞—É—Ç–æ–º
        try:
            if request.use_async:
                result = await asyncio.wait_for(
                    generate_response_async(request.model_id, request.query, request.max_length, request.temperature), 
                    timeout=config_manager.get_max_generation_time()
                )
            else:
                # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                loop = asyncio.get_event_loop()
                result = await asyncio.wait_for(
                    loop.run_in_executor(
                        thread_pool, 
                        model_service.model_repository.generate_text,
                        request.model_id, request.query, request.max_length, request.temperature
                    ),
                    timeout=config_manager.get_max_generation_time()
                )
            
            logger.info("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
        except asyncio.TimeoutError:
            logger.error("‚ùå –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–≤—ã—Å–∏–ª–∞ —Ç–∞–π–º–∞—É—Ç")
            raise HTTPException(status_code=408, detail="Generation timeout")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            raise HTTPException(status_code=500, detail=f"Generation error: {e}")
        
        duration = time.time() - start_time
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏
        memory_usage = model_service.get_memory_usage()
        
        response = ModelResponse(
            success=True,
            result=result,
            processing_time=duration,
            timestamp=datetime.now().isoformat(),
            model_id=request.model_id,
            memory_usage=memory_usage
        )
        
        return response
        
    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        
        return ModelResponse(
            success=False,
            result="",
            processing_time=duration,
            timestamp=datetime.now().isoformat(),
            error=str(e)
        )


@app.post("/generate-batch")
async def generate_batch_responses(requests: List[ModelRequest]):
    """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
    start_time = time.time()
    
    try:
        if model_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        tasks = []
        for req in requests:
            if req.use_async:
                task = asyncio.create_task(
                    asyncio.wait_for(
                        generate_response_async(req.model_id, req.query, req.max_length, req.temperature), 
                        timeout=60.0
                    )
                )
            else:
                loop = asyncio.get_event_loop()
                task = asyncio.create_task(
                    asyncio.wait_for(
                        loop.run_in_executor(
                            thread_pool, 
                            model_service.model_repository.generate_text,
                            req.model_id, req.query, req.max_length, req.temperature
                        ),
                        timeout=60.0
                    )
                )
            tasks.append(task)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        duration = time.time() - start_time
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã
        responses = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                responses.append(ModelResponse(
                    success=False,
                    result="",
                    processing_time=duration,
                    timestamp=datetime.now().isoformat(),
                    error=str(result)
                ))
            else:
                responses.append(ModelResponse(
                    success=True,
                    result=result,
                    processing_time=duration,
                    timestamp=datetime.now().isoformat(),
                    model_id=requests[i].model_id
                ))
        
        return {
            "success": True,
            "results": responses,
            "total_processed": len(requests),
            "total_time": duration
        }
        
    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/models", response_model=List[ModelInfo])
async def get_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π"""
    try:
        if model_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        models = model_service.model_repository.find_all()
        
        return [
            ModelInfo(
                model_id=model.id,
                name=model.name,
                type=model.type,
                device=model.device,
                is_loaded=model.is_loaded,
                memory_usage=model_service.get_memory_usage() if model.is_loaded else None
            )
            for model in models
        ]
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/model/{model_id}")
async def get_model_info(model_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
    try:
        if model_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        info = model_service.get_model_info(model_id)
        if not info:
            raise HTTPException(status_code=404, detail="Model not found")
        
        return info
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/optimize-memory")
async def optimize_memory():
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞–º—è—Ç—å"""
    try:
        if model_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(thread_pool, model_service.optimize_memory)
        
        return {
            "success": True,
            "message": "–ü–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/memory-usage")
async def get_memory_usage():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏"""
    try:
        if model_service is None:
            raise HTTPException(status_code=503, detail="Service not initialized")
        
        return model_service.get_memory_usage()
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞–º—è—Ç–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ AI Model Service")
    logger.info(f"üìä –ú–∞–∫—Å–∏–º—É–º –ø–æ—Ç–æ–∫–æ–≤: {MAX_WORKERS}")
    logger.info(f"üìä –ú–∞–∫—Å–∏–º—É–º –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: {MAX_PROCESSES}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    uvicorn.run(
        "ai_model_service:app",
        host="0.0.0.0", 
        port=8003,
        workers=1,  # –û–¥–∏–Ω –≤–æ—Ä–∫–µ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        loop="uvloop",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º uvloop –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        http="httptools",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º httptools –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        access_log=True,
        log_level="info"
    ) 